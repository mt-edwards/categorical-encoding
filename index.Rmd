---
title: "Categorical Encoding"
author: "Matthew Edwards"
date: "24/05/2021"
output: ioslides_presentation
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## Introduction

Many Machine Learning models require numerical predictors. Exceptions include tree-based algorithms (e.g. Random Forest, XGBoost). So categorical predictors are **encoded** into numerical predictors during **pre-processing**. 

When the number of levels of a categorical predictor is small, methods such as:

1. One-hot Encoding `sklearn.preprocessing.OneHotEncoder()`
2. Ordinal Encoding `sklearn.preprocessing.Ordinalencoder()`

are usually sufficient. However, when the number of levels of a categorical predictor is large, other methods are required.

## Categorical Encoding

The most common categorical encoding methods for a large number of levels are:

1. Mean Encoder
2. Target Encoder
3. M-Estimation Encoder
3. Likelihood Encoder
4. Hashing Encoder
5. Weight of Evidence Encoder
6. Entity Encoder

## Notation

Consider a data set $\{(x_1,y_1),\dots,(x_n,y_n)\}$ where $x_i$ is a categorical predictor and $y_i$ is a binary or continuous target variable. An arbitrary categorical level is denoted $c_k$.

- $n$ is the number of data points
- $\bar{y}$ is the mean of the target variables $y_i$
- $n_k$ is the number of data points where $x_i=c_k$
- $\bar{y}_k$ is the mean of the target variables $y_i$ where $x_i=c_k$

## Mean Encoder

The mean encoding method is for binary and continuous target variables. Each category level is encoded as:

$$
\text{MeanEncoder}(c_k)=\bar{y}_k
$$

The category level $c_k$ is encoded as the mean of the target variables $y_i$ where $x_i=c_k$.

## M-Estimate Encoder

The m-estimate encoding method combines mean encoding with a prior encoding. Each category level is encoded as:

$$
\text{TargetEncoder}(c_k)=\lambda_k\cdot\text{MeanEncoder}(c_k)+(1-\lambda_k)\cdot\bar{y}
$$

where

$$
\lambda_k=\frac{n_k}{\texttt{m}+n_k}
$$

```python
import category_encoders as ce

ce.m_estimate.MEstimateEncoder(m=1.0)
```

## Likelihood Encoder

The likelihood encoding method uses a model's predictions to encode each categorical level. Mean encoding and m-estimate encoding are special cases of likelihood encoding:

1. **Mean Encoder**: 
    + $Y_i\mid X_i=c_k\sim N(\mu_k,\sigma^2)$
    + $\text{MeanEncoder}(c_k)=\hat{\mu}_k$ (MLE)
2. **M-Estimate Encoder**: 
    + $Y_i\mid X_i=c_k\sim N(\mu_k,\sigma^2)$ and $\mu_k\sim N(\bar{y},1/m)$
    + $\text{MEstimateEncoder}(c_k)=\hat{\mu}_k$ (MAP)

## GLMM Encoding

The Generalised Linear Mixed Model (GLMM) encoding method uses the GLMM for likelihood encoding. For continuous target variables the GLMM model is defined as:

$$
Y_i\mid X_i=c_k,u_k\sim N(\mu+u_k,\sigma^2)\quad\text{and}\quad u_k\sim N(0, \tau^2)
$$

Each category level is encoded as:

$$
\text{GLMMEncoder}(c_k)=\hat{\mu}+\hat{u}_k
$$

```python
import category_encoders as ce

ce.glmm.GLMMEncoder()
```

## Hashing Encoder

The hash encoding method uses a $\texttt{hash_method}$ to transform $N_c$ categorical values to 

$$
N_n=2^\texttt{n_components}
$$ 

numerical (integer) values. Note that $N_n$ is often much smaller than $N_c$. The hash encoding method then one-hot encodes the integer values.

Some information is lost due to **collisions** when multiple categorical values are transformed to a single numeric value.

```python
import category_encoders as ce

ce.hashing.HashingEncoder(hash_method='md5', n_components=8)
```

## Weight of Evidence Encoder

The weight of evidence encoding method is for binary target variables. Each category level is encoded as:

$$
\text{WOEEncoder}(c_k)=\ln\left(\frac{\hat{p}(X=c_k\mid Y=1)}{\hat{p}(X=c_k\mid Y=0)}\right)
$$

where 

$$
\hat{p}(X=c_k\mid Y=j)=\frac{\sum_{i=1}^nI(x_i=c_k,y_i=j)}{\sum_{i=1}^nI(y_i=j)}
$$

```python
import category_encoders as ce

ce.woe.WOEEncoder()
```

## Entity Embeddings

The entity embedding method 'embeds' categorical predictors using neural networks. The method is analogous to word embedding in the field of natural language processing (e.g. Word2Vec).

There is currently no python module for entity embedding with the `Pipeline` and `ColumnTransformer` classes

## Overview.

1. **Likelihood Encoding**: dense, uncompressed
2. **Hashing Encoding**: sparse, compressed
3. **Weight of Evidence Encoding**: dense, uncompressed
4. **Entity Embedding**: dense, compressed
