---
title: "Categorical Encoding"
author: "Matthew Edwards"
date: "24/05/2021"
output: ioslides_presentation
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## Introduction

Many Machine Learning (ML) models require numerical predictors. Exceptions include tree-based algorithms (e.g. Random Forest, XGBoost). Consequently, categorical predictors are **encoded** into numerical predictors during **pre-processing**. 

When the number of levels of a categorical predictor is small, methods such as:

1. One-hot Encoding (`sklearn.preprocessing.OneHotEncoder()`) 
2. Ordinal Encoding (`sklearn.preprocessing.Ordinalencoder()`)

are usually sufficient. However, when the number of levels of a categorical predictor is large other methods are required.

## Categorical Encoding

The most common categorical encoding methods for a large number of levels are:

1. Mean Encoder
2. Target Encoder
3. M-Estimation and James-Stein Encoder
3. Likelihood Encoder
4. Hashing Encoder
5. Weight of Evidence (WOE) Encoder
6. Entity Encoder

## Notation

Consider a data set $\{(x_1,y_1),\dots,(x_n,y_n)\}$ where $x_i$ is a categorical predictor and $y_i$ is a binary or continuous target variable. An arbitrary categorical level is denoted $c_k$.

- $n$ is the number of data points
- $\bar{y}$ is the mean of the target variables $y_i$
- $n_k$ is the number of data points where $x_i=c_k$
- $\bar{y}_k$ is the mean of the target variables where $x_i=c_k$

## Mean Encoder

The mean encoding method is for binary and continuous target variables. Each category level is encoded as:

$$
\text{MeanEncoder}(c_k)=\bar{y}_k
$$

The category level $c_k$ is encoded as the mean of the target variables where $x_i=c_k$.

## M-Estimate Encoder

The m-estimate encoding method combines mean encoding with a prior encoding. Each category level is encoded as:

$$
\text{TargetEncoder}(c_k)=\lambda_k\cdot\text{MeanEncoder}(c_k)+(1-\lambda_k)\bar{y}
$$

where

$$
\lambda_k=\frac{n_k}{m+n_k}
$$

```python
import category_encoders as ce

ce.m_estimate.MEstimateEncoder(m=1.0)
```

## Likelihood Encoder

The likelihood encoding method uses a model's predictions to encode each categorical level. Mean encoding and m-estimate encoding are special cases of likelihood encoding for continuous target variables:

1. **Mean Encoding**: 
    + $Y_i\mid X_i=c_k\sim N(\mu_k,\sigma^2)$
    + $\text{MeanEncoder}(c_k)=\hat{\mu}_k$ (MLE)
2. **M-Estimate Encoding**: 
    + $Y_i\mid X_i=c_k\sim N(\mu_k,\sigma^2)$ and $\mu_k\sim N(\bar{y},\sigma^2/m)$
    + $\text{MEstimateEncoder}(c_k)=\hat{\mu}_k$ (MAP)

## GLMM Encoding

The GLMM encoding method uses the Generalised Linear Mixed Model for likelihood encoding:

$$
(Y_i\mid X_i=c_k)=\mu+b_k+\epsilon_{ik}
$$

where $b_k$ and $\epsilon_{ik}$ are independent zero-mean Gaussian random variables. Each category level is encoded as:

$$
\text{GLMMEncoder}(c_k)=\hat{\mu}+\hat{b}_k
$$

```python
import category_encoders as ce

ce.glmm.GLMMEncoder()
```

## Hashing Encoder

The hash encoding method uses a `hash_method` to transform $N_c$ categorical values to 

$$
N_n=2^\texttt{n_components}
$$ 

numerical (integer) values. Note that $N_n$ is often much smaller than $N_c$. The hash encoding method then one-hot encodes the integer values.

Some information is lost due to **collisions** when multiple categorical values are transformed to a single numeric value.

```python
import category_encoders as ce

ce.hashing.HashingEncoder(hash_method='md5', n_components=8)
```

## Weight of Evidence (WOE) Encoder

The WOE encoding method is for binary target variables. Each category value is encoded as:

$$
\text{WOEEncoder}(c_k)=\ln\left(\frac{\hat{p}(X=c_k\mid Y=1)}{\hat{p}(X=c_k\mid Y=0)}\right)
$$

where 

$$
\hat{p}(X=c_k\mid Y=j)=\frac{\sum_{j=1}^nI(X_j=c_k,Y_j=j)}{\sum_{j=1}^nI(Y=j)}
$$

```python
import category_encoders as ce

ce.woe.WOEEncoder()
```

## Entity Embeddings

The entity embedding method 'embeds' categorical predictors into a Euclidean space using neural networks. The method is analugous to word embedding in the field of natural language processing (e.g. Word2Vec).

There is currently no python module for entity embedding with the `Pipeline` and `ColumnTransformer` classes

## Overview.

1. **Likelihood Encoding**: dense, uncompressed
2. **Hashing Encoding**: sparse, compressed
3. **Weight of Evidence**: dense, uncompressed
4. **Entity Embedding**: dense, compressed
